# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kHvVlWbdRnN5WNYDjWKOS_R2kgvwL5_z
"""

!pip install --upgrade langchain openai -q
!pip install sentence_transformers -q
!pip install unstructured -q
!pip install unstructured[local-inference] -q
!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q
!apt-get install poppler-utils
!pip install pinecone-client

!pip install streamlit

!pip list | grep Pillow
!pip uninstall Pillow
!pip install Pillow

import streamlit as st
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Pinecone

# Set up the directory containing your custom data files
directory = '/content/data'

# Load the documents from the directory
def load_docs(directory):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    return documents

# Split the documents into smaller chunks for processing
def split_docs(documents, chunk_size=500, chunk_overlap=20):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    docs = text_splitter.split_documents(documents)
    return docs

# Load the documents from the directory
documents = load_docs(directory)
# Split the documents into smaller chunks
docs = split_docs(documents)

# Create the embeddings for the documents using SentenceTransformer
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# Set up Pinecone for vector storage and retrieval
import pinecone
from langchain.vectorstores import Pinecone
# initialize pinecone
pinecone.init(
    api_key="69b312ba-93e5-4620-b492-36fba2fc580c",  # find at app.pinecone.io
    environment="asia-southeast1-gcp-free"  # next to api key in console
)

index = Pinecone.from_documents(docs, embeddings, index_name=index_name)
index_name = "langchain-chatbot"
index = Pinecone.from_documents(documents, embeddings, index_name=index_name)

# Streamlit app
def main():
    st.title("Chatbot Answering from Your Own Knowledge Base")
    user_input = st.text_input("You: ", "")

    if user_input:
        response = get_bot_response(user_input)
        st.text("Chatbot: " + response)

# Function to get the chatbot response
def get_bot_response(query, k=1, score=False):
    query_result = embeddings.embed_query(query)
    if score:
        similar_docs = index.similarity_search_with_score(query_result, k=k)
    else:
        similar_docs = index.similarity_search(query_result, k=k)

    # Assuming the response is stored in the first document
    if similar_docs:
        response = similar_docs[0].page_content
        return response

    return "I'm sorry, I don't have an answer for that question."

if __name__ == "__main__":
    main()

